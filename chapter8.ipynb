{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "70. 単語埋め込みの読み込み\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o6IzA3LkNePZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OugkknLH6SuR",
        "outputId": "4f0bb5e9-39ae-42da-9d44-abb1609ddaae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "root",
        "outputId": "72f9eb7e-f649-4d3b-ef7b-cfe6ebd0de32"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word2id = {}\n",
        "id2word = {}\n",
        "embeddings = []\n",
        "\n",
        "EMB_DIM = 300\n",
        "word2id[\"<PAD>\"] = 0\n",
        "id2word[0] = \"<PAD>\"\n",
        "embeddings.append(np.zeros(EMB_DIM))\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/chapter8/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(\n",
        "    file_path,\n",
        "    binary=True\n",
        ")\n",
        "\n",
        "MAX_VOCAB = 300000\n",
        "\n",
        "for word in model.index_to_key:\n",
        "    if len(embeddings) >= MAX_VOCAB:\n",
        "        break\n",
        "\n",
        "    token_id = len(embeddings)\n",
        "    word2id[word] = token_id\n",
        "    id2word[token_id] = word\n",
        "    embeddings.append(model[word])\n",
        "\n",
        "\n",
        "E = np.vstack(embeddings)\n",
        "print(E.shape)\n",
        "print(word2id[\"<PAD>\"])\n",
        "print(E[0].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZZOMHgT62x0",
        "outputId": "f54fe521-b607-4e78-a3b5-afc40ba10a27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300000, 300)\n",
            "0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "71. データセットの読み込み\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EK4OdHFdTU_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0a5gkU4X8zER",
        "outputId": "eacb62b1-2fa5-4734-fd94-09b4ffd15ac6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "\n",
        "def load_sst_tsv(file_path, word2id):\n",
        "    data = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f, delimiter='\\t')\n",
        "\n",
        "        for row in reader:\n",
        "            text = row['sentence']\n",
        "            label = int(row['label'])\n",
        "\n",
        "            tokens = text.split()\n",
        "\n",
        "            input_ids = [\n",
        "                word2id[token]\n",
        "                for token in tokens\n",
        "                if token in word2id\n",
        "            ]\n",
        "\n",
        "            if len(input_ids) == 0:\n",
        "                continue\n",
        "\n",
        "            example = {\n",
        "                'text': text,\n",
        "                'label': torch.tensor([label], dtype=torch.float),\n",
        "                'input_ids': torch.tensor(input_ids, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "            data.append(example)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "train_path = '/content/drive/MyDrive/Colab Notebooks/chapter8/SST-2/train.tsv'\n",
        "dev_path = '/content/drive/MyDrive/Colab Notebooks/chapter8/SST-2/dev.tsv'\n",
        "\n",
        "train_data = load_sst_tsv(train_path, word2id)\n",
        "dev_data = load_sst_tsv(dev_path, word2id)\n",
        "\n",
        "print(len(train_data), len(dev_data))\n",
        "print(train_data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxOnBi_p-LuC",
        "outputId": "0e6b0389-64c4-48c9-9c11-b6bbdccc7255"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66396 872\n",
            "{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "72. Bag of wordsモデルの構築"
      ],
      "metadata": {
        "id": "HZfoW2VQEN7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_matrix = torch.tensor(E, dtype=torch.float)\n",
        "class BoWClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super().__init__()\n",
        "        # Convert numpy array to torch tensor if it's not already one\n",
        "        if isinstance(embedding_matrix, np.ndarray):\n",
        "            embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float)\n",
        "\n",
        "        vocab_size, emb_dim = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            vocab_size,\n",
        "            emb_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        self.embedding.weight.data.copy_(embedding_matrix)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.fc = nn.Linear(emb_dim, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        embeds = self.embedding(input_ids)\n",
        "        avg_embeds = embeds.mean(dim=1)\n",
        "        logits = self.fc(avg_embeds)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Hi-rKoNPEPW8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "73. モデルの学習"
      ],
      "metadata": {
        "id": "K9b7MDQ0G5qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn # Ensure nn is imported\n",
        "import torch.optim as optim # Ensure optim is imported\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [x['text'] for x in batch]\n",
        "    labels = torch.stack([x['label'] for x in batch])\n",
        "\n",
        "    input_ids = [x['input_ids'] for x in batch]\n",
        "    max_len = max(len(ids) for ids in input_ids)\n",
        "\n",
        "    padded = []\n",
        "    for ids in input_ids:\n",
        "        pad_len = max_len - len(ids)\n",
        "        padded.append(\n",
        "            torch.cat([ids, torch.zeros(pad_len, dtype=torch.long)])\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        'text': texts,\n",
        "        'input_ids': torch.stack(padded),\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "model = BoWClassifier(embedding_matrix)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(batch['input_ids'])\n",
        "        loss = criterion(logits, batch['label'])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{EPOCHS}], \"\n",
        "                f\"Step [{i+1}/{len(train_loader)}], \"\n",
        "                f\"Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} finished. Avg Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmK2p5Y_G7F9",
        "outputId": "c4d91011-c256-4e58-e62c-90ef78739517"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/2075], Loss: 0.6875\n",
            "Epoch [1/5], Step [200/2075], Loss: 0.6599\n",
            "Epoch [1/5], Step [300/2075], Loss: 0.6768\n",
            "Epoch [1/5], Step [400/2075], Loss: 0.6170\n",
            "Epoch [1/5], Step [500/2075], Loss: 0.6393\n",
            "Epoch [1/5], Step [600/2075], Loss: 0.6443\n",
            "Epoch [1/5], Step [700/2075], Loss: 0.6387\n",
            "Epoch [1/5], Step [800/2075], Loss: 0.6581\n",
            "Epoch [1/5], Step [900/2075], Loss: 0.6307\n",
            "Epoch [1/5], Step [1000/2075], Loss: 0.5545\n",
            "Epoch [1/5], Step [1100/2075], Loss: 0.6460\n",
            "Epoch [1/5], Step [1200/2075], Loss: 0.6502\n",
            "Epoch [1/5], Step [1300/2075], Loss: 0.5625\n",
            "Epoch [1/5], Step [1400/2075], Loss: 0.6101\n",
            "Epoch [1/5], Step [1500/2075], Loss: 0.6074\n",
            "Epoch [1/5], Step [1600/2075], Loss: 0.6184\n",
            "Epoch [1/5], Step [1700/2075], Loss: 0.5808\n",
            "Epoch [1/5], Step [1800/2075], Loss: 0.4945\n",
            "Epoch [1/5], Step [1900/2075], Loss: 0.5950\n",
            "Epoch [1/5], Step [2000/2075], Loss: 0.6088\n",
            "Epoch 1 finished. Avg Loss: 0.6196\n",
            "Epoch [2/5], Step [100/2075], Loss: 0.4839\n",
            "Epoch [2/5], Step [200/2075], Loss: 0.5536\n",
            "Epoch [2/5], Step [300/2075], Loss: 0.5670\n",
            "Epoch [2/5], Step [400/2075], Loss: 0.5075\n",
            "Epoch [2/5], Step [500/2075], Loss: 0.5269\n",
            "Epoch [2/5], Step [600/2075], Loss: 0.5340\n",
            "Epoch [2/5], Step [700/2075], Loss: 0.5608\n",
            "Epoch [2/5], Step [800/2075], Loss: 0.5242\n",
            "Epoch [2/5], Step [900/2075], Loss: 0.5772\n",
            "Epoch [2/5], Step [1000/2075], Loss: 0.5970\n",
            "Epoch [2/5], Step [1100/2075], Loss: 0.5141\n",
            "Epoch [2/5], Step [1200/2075], Loss: 0.6279\n",
            "Epoch [2/5], Step [1300/2075], Loss: 0.6154\n",
            "Epoch [2/5], Step [1400/2075], Loss: 0.4938\n",
            "Epoch [2/5], Step [1500/2075], Loss: 0.5211\n",
            "Epoch [2/5], Step [1600/2075], Loss: 0.5540\n",
            "Epoch [2/5], Step [1700/2075], Loss: 0.5011\n",
            "Epoch [2/5], Step [1800/2075], Loss: 0.5153\n",
            "Epoch [2/5], Step [1900/2075], Loss: 0.5753\n",
            "Epoch [2/5], Step [2000/2075], Loss: 0.5635\n",
            "Epoch 2 finished. Avg Loss: 0.5360\n",
            "Epoch [3/5], Step [100/2075], Loss: 0.4947\n",
            "Epoch [3/5], Step [200/2075], Loss: 0.5206\n",
            "Epoch [3/5], Step [300/2075], Loss: 0.4493\n",
            "Epoch [3/5], Step [400/2075], Loss: 0.3982\n",
            "Epoch [3/5], Step [500/2075], Loss: 0.5468\n",
            "Epoch [3/5], Step [600/2075], Loss: 0.4728\n",
            "Epoch [3/5], Step [700/2075], Loss: 0.5477\n",
            "Epoch [3/5], Step [800/2075], Loss: 0.4362\n",
            "Epoch [3/5], Step [900/2075], Loss: 0.4896\n",
            "Epoch [3/5], Step [1000/2075], Loss: 0.4969\n",
            "Epoch [3/5], Step [1100/2075], Loss: 0.5585\n",
            "Epoch [3/5], Step [1200/2075], Loss: 0.4518\n",
            "Epoch [3/5], Step [1300/2075], Loss: 0.4044\n",
            "Epoch [3/5], Step [1400/2075], Loss: 0.5217\n",
            "Epoch [3/5], Step [1500/2075], Loss: 0.5755\n",
            "Epoch [3/5], Step [1600/2075], Loss: 0.5681\n",
            "Epoch [3/5], Step [1700/2075], Loss: 0.5506\n",
            "Epoch [3/5], Step [1800/2075], Loss: 0.4836\n",
            "Epoch [3/5], Step [1900/2075], Loss: 0.5664\n",
            "Epoch [3/5], Step [2000/2075], Loss: 0.5403\n",
            "Epoch 3 finished. Avg Loss: 0.4935\n",
            "Epoch [4/5], Step [100/2075], Loss: 0.4781\n",
            "Epoch [4/5], Step [200/2075], Loss: 0.4687\n",
            "Epoch [4/5], Step [300/2075], Loss: 0.4495\n",
            "Epoch [4/5], Step [400/2075], Loss: 0.4328\n",
            "Epoch [4/5], Step [500/2075], Loss: 0.4891\n",
            "Epoch [4/5], Step [600/2075], Loss: 0.4649\n",
            "Epoch [4/5], Step [700/2075], Loss: 0.4924\n",
            "Epoch [4/5], Step [800/2075], Loss: 0.4232\n",
            "Epoch [4/5], Step [900/2075], Loss: 0.5249\n",
            "Epoch [4/5], Step [1000/2075], Loss: 0.4944\n",
            "Epoch [4/5], Step [1100/2075], Loss: 0.5915\n",
            "Epoch [4/5], Step [1200/2075], Loss: 0.3980\n",
            "Epoch [4/5], Step [1300/2075], Loss: 0.4485\n",
            "Epoch [4/5], Step [1400/2075], Loss: 0.4433\n",
            "Epoch [4/5], Step [1500/2075], Loss: 0.4652\n",
            "Epoch [4/5], Step [1600/2075], Loss: 0.4847\n",
            "Epoch [4/5], Step [1700/2075], Loss: 0.4960\n",
            "Epoch [4/5], Step [1800/2075], Loss: 0.4997\n",
            "Epoch [4/5], Step [1900/2075], Loss: 0.5879\n",
            "Epoch [4/5], Step [2000/2075], Loss: 0.3567\n",
            "Epoch 4 finished. Avg Loss: 0.4679\n",
            "Epoch [5/5], Step [100/2075], Loss: 0.5805\n",
            "Epoch [5/5], Step [200/2075], Loss: 0.4811\n",
            "Epoch [5/5], Step [300/2075], Loss: 0.4048\n",
            "Epoch [5/5], Step [400/2075], Loss: 0.5481\n",
            "Epoch [5/5], Step [500/2075], Loss: 0.5041\n",
            "Epoch [5/5], Step [600/2075], Loss: 0.3981\n",
            "Epoch [5/5], Step [700/2075], Loss: 0.4023\n",
            "Epoch [5/5], Step [800/2075], Loss: 0.4432\n",
            "Epoch [5/5], Step [900/2075], Loss: 0.4380\n",
            "Epoch [5/5], Step [1000/2075], Loss: 0.4653\n",
            "Epoch [5/5], Step [1100/2075], Loss: 0.5458\n",
            "Epoch [5/5], Step [1200/2075], Loss: 0.4449\n",
            "Epoch [5/5], Step [1300/2075], Loss: 0.3915\n",
            "Epoch [5/5], Step [1400/2075], Loss: 0.3620\n",
            "Epoch [5/5], Step [1500/2075], Loss: 0.4257\n",
            "Epoch [5/5], Step [1600/2075], Loss: 0.4059\n",
            "Epoch [5/5], Step [1700/2075], Loss: 0.5119\n",
            "Epoch [5/5], Step [1800/2075], Loss: 0.5104\n",
            "Epoch [5/5], Step [1900/2075], Loss: 0.3563\n",
            "Epoch [5/5], Step [2000/2075], Loss: 0.4680\n",
            "Epoch 5 finished. Avg Loss: 0.4508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "74. モデルの評価\n",
        "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
      ],
      "metadata": {
        "id": "0xaHN4sFHWZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            logits = model(batch['input_ids'])\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            correct += (preds == batch['label']).sum().item()\n",
        "            total += batch['label'].size(0)\n",
        "\n",
        "    return correct / total\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "    dev_data,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "dev_acc = evaluate_accuracy(model, dev_loader)\n",
        "print(f\"Dev Accuracy: {dev_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Wkw1GKHX_P",
        "outputId": "b4c78b0b-7202-4c64-9a7c-0a6e5b38f046"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Accuracy: 0.7787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "75. パディング"
      ],
      "metadata": {
        "id": "3pPENUVVHnu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate(batch):\n",
        "    # 1. 按 token 长度从长到短排序\n",
        "    batch = sorted(batch, key=lambda x: len(x['input_ids']), reverse=True)\n",
        "\n",
        "    # 2. 取最长序列长度\n",
        "    max_len = len(batch[0]['input_ids'])\n",
        "\n",
        "    padded_input_ids = []\n",
        "    labels = []\n",
        "\n",
        "    # 3. padding\n",
        "    for example in batch:\n",
        "        input_ids = example['input_ids']\n",
        "        pad_len = max_len - len(input_ids)\n",
        "\n",
        "        padded_ids = torch.cat(\n",
        "            [input_ids, torch.zeros(pad_len, dtype=torch.long)]\n",
        "        )\n",
        "\n",
        "        padded_input_ids.append(padded_ids)\n",
        "        labels.append(example['label'])\n",
        "\n",
        "    # 4. stack 成 tensor\n",
        "    return {\n",
        "        'input_ids': torch.stack(padded_input_ids),\n",
        "        'label': torch.stack(labels)\n",
        "    }\n",
        "batch = train_data[:4]\n",
        "out = collate(batch)\n",
        "\n",
        "print(out['input_ids'])\n",
        "print(out['label'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhuMv6EIHp3i",
        "outputId": "da592015-a4fb-416c-9d9c-50c9272eb52e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,\n",
            "           1276,   1964],\n",
            "        [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,\n",
            "              0,      0],\n",
            "        [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,\n",
            "              0,      0],\n",
            "        [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,\n",
            "              0,      0]])\n",
            "tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "76. ミニバッチ学習"
      ],
      "metadata": {
        "id": "AnDv_TJNH1cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate\n",
        ")\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "    dev_data,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate\n",
        ")\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(batch['input_ids'])\n",
        "        loss = criterion(logits, batch['label'])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
        "                f\"Step [{i+1}/{len(train_loader)}] \"\n",
        "                f\"Loss: {loss.item():.4f}\"\n",
        "            )\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} finished. Avg Loss: {avg_loss:.4f}\")\n",
        "def evaluate_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            logits = model(batch['input_ids'])\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            correct += (preds == batch['label']).sum().item()\n",
        "            total += batch['label'].size(0)\n",
        "\n",
        "    return correct / total\n",
        "dev_acc = evaluate_accuracy(model, dev_loader)\n",
        "print(f\"Dev Accuracy: {dev_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFkKXQCiH16Y",
        "outputId": "4a9e89f3-a9fe-4b6c-a2d5-cc38ef5cf05c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] Step [100/2075] Loss: 0.3536\n",
            "Epoch [1/5] Step [200/2075] Loss: 0.3945\n",
            "Epoch [1/5] Step [300/2075] Loss: 0.5544\n",
            "Epoch [1/5] Step [400/2075] Loss: 0.4982\n",
            "Epoch [1/5] Step [500/2075] Loss: 0.3710\n",
            "Epoch [1/5] Step [600/2075] Loss: 0.3271\n",
            "Epoch [1/5] Step [700/2075] Loss: 0.4676\n",
            "Epoch [1/5] Step [800/2075] Loss: 0.5052\n",
            "Epoch [1/5] Step [900/2075] Loss: 0.3585\n",
            "Epoch [1/5] Step [1000/2075] Loss: 0.4435\n",
            "Epoch [1/5] Step [1100/2075] Loss: 0.4646\n",
            "Epoch [1/5] Step [1200/2075] Loss: 0.5190\n",
            "Epoch [1/5] Step [1300/2075] Loss: 0.3942\n",
            "Epoch [1/5] Step [1400/2075] Loss: 0.4658\n",
            "Epoch [1/5] Step [1500/2075] Loss: 0.4435\n",
            "Epoch [1/5] Step [1600/2075] Loss: 0.3105\n",
            "Epoch [1/5] Step [1700/2075] Loss: 0.4259\n",
            "Epoch [1/5] Step [1800/2075] Loss: 0.4721\n",
            "Epoch [1/5] Step [1900/2075] Loss: 0.3741\n",
            "Epoch [1/5] Step [2000/2075] Loss: 0.4527\n",
            "Epoch 1 finished. Avg Loss: 0.4403\n",
            "Epoch [2/5] Step [100/2075] Loss: 0.4975\n",
            "Epoch [2/5] Step [200/2075] Loss: 0.3764\n",
            "Epoch [2/5] Step [300/2075] Loss: 0.4691\n",
            "Epoch [2/5] Step [400/2075] Loss: 0.3451\n",
            "Epoch [2/5] Step [500/2075] Loss: 0.6147\n",
            "Epoch [2/5] Step [600/2075] Loss: 0.4207\n",
            "Epoch [2/5] Step [700/2075] Loss: 0.3477\n",
            "Epoch [2/5] Step [800/2075] Loss: 0.4276\n",
            "Epoch [2/5] Step [900/2075] Loss: 0.3620\n",
            "Epoch [2/5] Step [1000/2075] Loss: 0.4452\n",
            "Epoch [2/5] Step [1100/2075] Loss: 0.3780\n",
            "Epoch [2/5] Step [1200/2075] Loss: 0.4292\n",
            "Epoch [2/5] Step [1300/2075] Loss: 0.4342\n",
            "Epoch [2/5] Step [1400/2075] Loss: 0.2734\n",
            "Epoch [2/5] Step [1500/2075] Loss: 0.3805\n",
            "Epoch [2/5] Step [1600/2075] Loss: 0.3550\n",
            "Epoch [2/5] Step [1700/2075] Loss: 0.3662\n",
            "Epoch [2/5] Step [1800/2075] Loss: 0.4937\n",
            "Epoch [2/5] Step [1900/2075] Loss: 0.3920\n",
            "Epoch [2/5] Step [2000/2075] Loss: 0.4151\n",
            "Epoch 2 finished. Avg Loss: 0.4324\n",
            "Epoch [3/5] Step [100/2075] Loss: 0.4138\n",
            "Epoch [3/5] Step [200/2075] Loss: 0.5623\n",
            "Epoch [3/5] Step [300/2075] Loss: 0.3966\n",
            "Epoch [3/5] Step [400/2075] Loss: 0.4318\n",
            "Epoch [3/5] Step [500/2075] Loss: 0.4251\n",
            "Epoch [3/5] Step [600/2075] Loss: 0.2540\n",
            "Epoch [3/5] Step [700/2075] Loss: 0.3922\n",
            "Epoch [3/5] Step [800/2075] Loss: 0.4651\n",
            "Epoch [3/5] Step [900/2075] Loss: 0.3625\n",
            "Epoch [3/5] Step [1000/2075] Loss: 0.4121\n",
            "Epoch [3/5] Step [1100/2075] Loss: 0.2898\n",
            "Epoch [3/5] Step [1200/2075] Loss: 0.4344\n",
            "Epoch [3/5] Step [1300/2075] Loss: 0.3699\n",
            "Epoch [3/5] Step [1400/2075] Loss: 0.4291\n",
            "Epoch [3/5] Step [1500/2075] Loss: 0.3337\n",
            "Epoch [3/5] Step [1600/2075] Loss: 0.5383\n",
            "Epoch [3/5] Step [1700/2075] Loss: 0.4312\n",
            "Epoch [3/5] Step [1800/2075] Loss: 0.3543\n",
            "Epoch [3/5] Step [1900/2075] Loss: 0.5111\n",
            "Epoch [3/5] Step [2000/2075] Loss: 0.3793\n",
            "Epoch 3 finished. Avg Loss: 0.4253\n",
            "Epoch [4/5] Step [100/2075] Loss: 0.3972\n",
            "Epoch [4/5] Step [200/2075] Loss: 0.3603\n",
            "Epoch [4/5] Step [300/2075] Loss: 0.3939\n",
            "Epoch [4/5] Step [400/2075] Loss: 0.3871\n",
            "Epoch [4/5] Step [500/2075] Loss: 0.3445\n",
            "Epoch [4/5] Step [600/2075] Loss: 0.4569\n",
            "Epoch [4/5] Step [700/2075] Loss: 0.4050\n",
            "Epoch [4/5] Step [800/2075] Loss: 0.3784\n",
            "Epoch [4/5] Step [900/2075] Loss: 0.5353\n",
            "Epoch [4/5] Step [1000/2075] Loss: 0.3191\n",
            "Epoch [4/5] Step [1100/2075] Loss: 0.4831\n",
            "Epoch [4/5] Step [1200/2075] Loss: 0.3833\n",
            "Epoch [4/5] Step [1300/2075] Loss: 0.4719\n",
            "Epoch [4/5] Step [1400/2075] Loss: 0.3659\n",
            "Epoch [4/5] Step [1500/2075] Loss: 0.2937\n",
            "Epoch [4/5] Step [1600/2075] Loss: 0.3442\n",
            "Epoch [4/5] Step [1700/2075] Loss: 0.4126\n",
            "Epoch [4/5] Step [1800/2075] Loss: 0.6300\n",
            "Epoch [4/5] Step [1900/2075] Loss: 0.4747\n",
            "Epoch [4/5] Step [2000/2075] Loss: 0.4682\n",
            "Epoch 4 finished. Avg Loss: 0.4206\n",
            "Epoch [5/5] Step [100/2075] Loss: 0.6471\n",
            "Epoch [5/5] Step [200/2075] Loss: 0.4500\n",
            "Epoch [5/5] Step [300/2075] Loss: 0.5890\n",
            "Epoch [5/5] Step [400/2075] Loss: 0.3208\n",
            "Epoch [5/5] Step [500/2075] Loss: 0.3674\n",
            "Epoch [5/5] Step [600/2075] Loss: 0.2884\n",
            "Epoch [5/5] Step [700/2075] Loss: 0.4819\n",
            "Epoch [5/5] Step [800/2075] Loss: 0.4509\n",
            "Epoch [5/5] Step [900/2075] Loss: 0.4365\n",
            "Epoch [5/5] Step [1000/2075] Loss: 0.4509\n",
            "Epoch [5/5] Step [1100/2075] Loss: 0.4180\n",
            "Epoch [5/5] Step [1200/2075] Loss: 0.4696\n",
            "Epoch [5/5] Step [1300/2075] Loss: 0.3382\n",
            "Epoch [5/5] Step [1400/2075] Loss: 0.6151\n",
            "Epoch [5/5] Step [1500/2075] Loss: 0.3651\n",
            "Epoch [5/5] Step [1600/2075] Loss: 0.4149\n",
            "Epoch [5/5] Step [1700/2075] Loss: 0.3975\n",
            "Epoch [5/5] Step [1800/2075] Loss: 0.3609\n",
            "Epoch [5/5] Step [1900/2075] Loss: 0.3692\n",
            "Epoch [5/5] Step [2000/2075] Loss: 0.3981\n",
            "Epoch 5 finished. Avg Loss: 0.4166\n",
            "Dev Accuracy: 0.7867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "77. GPU上での学習\n"
      ],
      "metadata": {
        "id": "mJuy-yUPH2VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "def evaluate_accuracy_gpu(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(input_ids)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "dev_acc = evaluate_accuracy_gpu(model, dev_loader, device)\n",
        "print(f\"Dev Accuracy (GPU): {dev_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAZf62GlH2od",
        "outputId": "bb5a742b-1436-46c3-ec6f-d0f979fe13a9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4141\n",
            "Epoch 2, Loss: 0.4106\n",
            "Epoch 3, Loss: 0.4087\n",
            "Epoch 4, Loss: 0.4062\n",
            "Epoch 5, Loss: 0.4047\n",
            "Dev Accuracy (GPU): 0.7913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "78. 単語埋め込みのファインチューニング\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cPf77sr18vda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"[Fine-tune] Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "dev_acc = evaluate_accuracy_gpu(model, dev_loader, device)\n",
        "print(f\"Dev Accuracy (Fine-tuning): {dev_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqitK8cRIKzU",
        "outputId": "8d54b2a8-0134-4f76-a8ef-4f13bbf15cf6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fine-tune] Epoch 1, Loss: 0.6655\n",
            "[Fine-tune] Epoch 2, Loss: 0.5650\n",
            "[Fine-tune] Epoch 3, Loss: 0.4601\n",
            "[Fine-tune] Epoch 4, Loss: 0.3890\n",
            "[Fine-tune] Epoch 5, Loss: 0.3422\n",
            "Dev Accuracy (Fine-tuning): 0.8096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "79. アーキテクチャの変更\n"
      ],
      "metadata": {
        "id": "wsRc9yuEIJwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BoWMLPClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super().__init__()\n",
        "        V, D = embedding_matrix.shape\n",
        "\n",
        "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(\n",
        "    torch.from_numpy(embedding_matrix)\n",
        ")\n",
        "\n",
        "        self.embedding.weight.requires_grad = False  # 是否 fine-tune 可选\n",
        "\n",
        "        self.fc1 = nn.Linear(D, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        emb = self.embedding(input_ids)        # (B, L, D)\n",
        "        avg_emb = emb.mean(dim=1)               # (B, D)\n",
        "\n",
        "        h = self.relu(self.fc1(avg_emb))        # (B, 128)\n",
        "        logits = self.fc2(h)                    # (B, 1)\n",
        "        return logits\n",
        "model = BoWMLPClassifier(E).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "dev_acc = evaluate_accuracy_gpu(model, dev_loader, device)\n",
        "print(f\"Dev Accuracy (MLP): {dev_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTBv7FkzbnU2",
        "outputId": "51397081-0ed1-4c29-9a84-47b39441395a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Accuracy (MLP): 0.4908\n"
          ]
        }
      ]
    }
  ]
}